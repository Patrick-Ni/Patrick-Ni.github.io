## 前置知识

#### 1、NLP任务

根据判断主题的级别, 将所有的NLP任务分为两种类型:

- **token-level task**: token级别的任务. 如**完形填空**(Cloze), 预测句子中某个位置的单词; 或者**实体识别**; 或是**词性标注**; **SQuAD**等.
- **sequence-level task**: 序列级别的任务, 也可以理解为句子级别的任务. 如**情感分类**等各种句子分类问题; 推断两个句子的是否是同义等.

[nlp任务](https://www.cnblogs.com/databingo/p/10182663.html)

#### 2、BERT

- [十分钟读懂谷歌BERT模型](https://zhuanlan.zhihu.com/p/51413773)
- [bert](https://www.cnblogs.com/anai/p/11645953.html)

#### 3、zero-shot one-shot

**zero-shot**：利用训练集数据训练模型，使得模型能够对测试集的对象进行分类，但是训练集类别和测试集类别之间没有交集；期间需要借助类别的描述，来建立训练集和测试集之间的联系，从而使得模型有效。[zero-shot](https://zhuanlan.zhihu.com/p/34656727)

**one-shot**：

#### 4、BERT四大下游任务

- **句子对分类任务**
- **单句子分类任务**
- **问答任务**
- **单句子标注任务**

[bert四大下游任务](https://zhuanlan.zhihu.com/p/102208639)

#### 5、几个标记

- [CLS] 标志放在第一个句子的首位，经过 BERT 得到的的表征向量 C 可以用于后续的分类任务。
- [SEP] 标志用于分开两个输入句子，例如输入句子 A 和 B，要在句子 A，B 后面增加 [SEP] 标志。
- [UNK]标志指的是未知字符
- [MASK] 标志用于遮盖句子中的一些单词，将单词用 [MASK] 遮盖之后，再利用 BERT 输出的 [MASK] 向量预测单词是什么。
- 例如给定两个句子 "my dog is cute" 和 "he likes palying" 作为输入样本，BERT 会转为 "[CLS] my dog is cute [SEP] he likes play ##ing [SEP]"。BERT 里面用了 WordPiece 方法，会将单词拆成子词单元 (SubWord)，所以有的词会拆出词根，例如 "palying" 会变成 "paly" + "##ing"。

[bert模型中的[CLS]、[UNK]、[SEP]](https://www.jianshu.com/p/46cb208d45c3)

#### 6、语言模型 language modeling

- 标准定义：对于语言序列 ![[公式]](https://www.zhihu.com/equation?tex=w_1%2Cw_2%2C...%2Cw_n)，语言模型就是计算该序列的概率，即 ![[公式]](https://www.zhihu.com/equation?tex=P%28w_1%2C+w_2%2C+...%2Cw_n%29) 。
- 从机器学习的角度来看：语言模型是对语句的**概率分布**的建模。
- 通俗解释：判断一个语言序列是否是正常语句，即是否是**人话，**例如 ![[公式]](https://www.zhihu.com/equation?tex=P%28I+%5C+am+%5C+Light%29%3EP%28Light+%5C+I+%5C+am%29) 。

https://zhuanlan.zhihu.com/p/52061158

#### 7、prompt learning

prompt是提示的意思，也就是说需要提示模型我们想让它干什么。通常在GPT-3中，我们输入一段描述，再加上“翻译”或者“问答”的prompt，那么GPT-3会生成相应的结果。最近该玩法在NLU中也得到了应用，比如情感分类任务，给定一句话“**I missed the bus today.**”，在其之后添加一个prompt：“**I felt so __**”，之后让语言模型用一个情感类的词进行完型填空，再将填空的词语映射到标签，这样一来就能够解决分类任务了。

[Prompt Learning-使用模板激发语言模型潜能](https://blog.csdn.net/qq_27590277/article/details/119657972)

#### 8、GPT模型

生成式的预训练

[GPT](https://www.cnblogs.com/yifanrensheng/p/13167796.html)

#### 9、Prompt Tuing

自动构建模板

- [P-Tuing](https://spaces.ac.cn/archives/8295/comment-page-1)

- [Prompt Learning使用模板激发语言模型潜能](https://blog.csdn.net/qq_27590277/article/details/119657972)

#### 10、几个预训练的语言模型（Pre-trained language models）

- seq2seq
- transformer
- GPT

[plm](https://blog.csdn.net/oldyang95/article/details/90079976)

#### 11、前向传播算法和反向传播算法

- Forward propagation
- Back propagation

[link](https://blog.csdn.net/bitcarmanlee/article/details/78819025)

#### 12、微调

https://www.cnblogs.com/xiaoyh/p/11735686.html

#### 13、实体粒度

https://blog.csdn.net/weixin_33022901/article/details/112106575